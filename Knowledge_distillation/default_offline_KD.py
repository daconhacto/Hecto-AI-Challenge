# ë¯¸ë¦¬ ìƒì„±í•´ë‘” logit.csvë“¤ì„ í†µí•´ KDë¥¼ ìˆ˜í–‰.
# ë¯¸ë¦¬ ìƒì„±í•´ë‘” ê±¸ë¡œ ìˆ˜í–‰í•˜ê¸° ë•Œë¬¸ì— augmentationì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŒì— ì£¼ì˜

import os
import gc
import sys
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
import json
import pprint
import random
import pandas as pd
import numpy as np
from PIL import Image
from tqdm import tqdm
import json # class_names ì €ì¥ì„ ìœ„í•´ ì¶”ê°€
from sklearn.model_selection import StratifiedKFold
import torch
from torch.utils.data import Dataset, DataLoader
import torchvision.transforms.functional as TF
import timm
import torchvision.transforms as transforms
from torchvision.transforms import v2
import torch.nn.functional as F
from torch import nn, optim
from sklearn.metrics import log_loss
from collections import defaultdict
import albumentations as A
from albumentations.pytorch import ToTensorV2
from utils import *
from dataset import *
from model import *

# Device Setting
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Hyperparameter Setting
CFG = {
    "WORK_DIR": '/home/sh/hecto/KD_test_folder/work_dirs/offline_KD_test_1', # train.pyë¡œ ìƒì„±ëœ work_directory
    "ROOT": '/home/sh/hecto/hecto_datasets/train_v1+v3', # data_path
    "START_FROM": None,

    # ë°˜ë“œì‹œ ì œê³µë˜ì–´ì•¼í•¨. í˜„ì¬ ëª¨ë¸ì´ ë°˜ë“œì‹œ workdir í´ë” ì•„ë˜ì— ìœ„ì¹˜í•´ ìˆëŠ” ê²Œ ë³´ì¥ì€ ì•ˆë˜ëŠ” ê±° ê°™ê¸´ í•œë°
    # ì¼ë‹¨ settings.jsonì—ì„œ ëª¨ë¸ëª…, ì´ë¯¸ì§€ ì‚¬ì´ì¦ˆë§Œ ë½‘ì•„ì˜¤ëŠ”ê±°ê³ , ë‹¤ë¥¸ ì •ë³´ëŠ” ì‚¬ìš©í•˜ì§„ ì•Šì•„ì„œ ê´œì°®ì„ë“¯ í•¨
    "TEACHER_MODEL_LOGITS_CSVS": [
        '/home/sh/hecto/best_ensemble_models_3/1089/train_logits.csv',
        '/home/sh/hecto/best_ensemble_models_3/convnext(1218)_fold1/train_logits.csv',
        '/home/sh/hecto/best_ensemble_models_3/EVA_large(12139)_fold1/train_logits.csv'
    ],
    "TEACHER_VAL_LOSSES": [
        0.1089,
        0.1218,
        0.12139
    ],

    'IMG_SIZE': 512,
    'BATCH_SIZE': 32, # í•™ìŠµ ì‹œ ë°°ì¹˜ í¬ê¸°
    'EPOCHS': 30,
    'SEED' : 42,
    'MODEL_NAME': 'convnext_base.fb_in22k_ft_in1k_384', # ì‚¬ìš©í•  ëª¨ë¸ ì´ë¦„
    'N_FOLDS': 5,
    'EARLY_STOPPING_PATIENCE': 3,
    'RUN_SINGLE_FOLD': True,  # Trueë¡œ ì„¤ì • ì‹œ íŠ¹ì • í´ë“œë§Œ ì‹¤í–‰
    'TARGET_FOLD': 1,          # RUN_SINGLE_FOLDê°€ Trueì¼ ë•Œ ì‹¤í–‰í•  í´ë“œ ë²ˆí˜¸ (1-based)
    
    # KD LOSS
    'KD_PARAMS': {
        'alpha': 0.7,
        'temperature': 4.0
    },
    
    # ìƒˆë¡­ê²Œ ì¶”ê°€ëœ loggingíŒŒíŠ¸. classì˜ ê²½ìš° ë¬´ì¡°ê±´ í’€ê²½ë¡œë¡œ ì ì–´ì•¼í•©ë‹ˆë‹¤. nn.CrossEntropyLoss ì²˜ëŸ¼ ì ìœ¼ë©´ ì˜¤ë¥˜ë‚¨
    'LOSS': {
        'class': 'torch.nn.CrossEntropyLoss',
        'params': {}   
    },
    'OPTIMIZER': {
        'class': 'torch.optim.AdamW',
        'params': {
            'lr': 1e-4,
            'weight_decay': 1e-2
        }
    },
    'SCHEDULER': {
        'class': 'torch.optim.lr_scheduler.CosineAnnealingLR',
        'params': {
            'T_max': 30,
            'eta_min': 1e-7
        }
    },
}

# --- Albumentations ê¸°ë°˜ ì´ë¯¸ì§€ ë³€í™˜ ì •ì˜ ---
CFG['IMG_SIZE'] = CFG['IMG_SIZE'] if isinstance(CFG['IMG_SIZE'], tuple) else (CFG['IMG_SIZE'], CFG['IMG_SIZE'])
train_transform = A.Compose([
    A.Resize(CFG['IMG_SIZE'][0], CFG['IMG_SIZE'][1]),
    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),
    ToTensorV2()
])

val_transform = A.Compose([
    A.Resize(CFG['IMG_SIZE'][0], CFG['IMG_SIZE'][1]),
    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),
    ToTensorV2()
])


def loss_fn_kd(outputs, labels, teacher_outputs, params):
    """
    Compute the knowledge-distillation (KD) loss given outputs, labels.
    "Hyperparameters": temperature and alpha

    NOTE: the KL Divergence for PyTorch comparing the softmaxs of teacher
    and student expects the input tensor to be log probabilities! See Issue #2
    """
    alpha = params['alpha']
    T = params['temperature']
    KD_loss = nn.KLDivLoss()(F.log_softmax(outputs/T, dim=1),
                             F.softmax(teacher_outputs/T, dim=1)) * (alpha * T * T) + \
              F.cross_entropy(outputs, labels) * (1. - alpha)

    return KD_loss


def train_main():
    # work directory ìƒì„±
    work_dir = CFG['WORK_DIR']
    os.makedirs(work_dir, exist_ok=True)
    
    # logger
    sys.stdout = Logger(os.path.join(work_dir, "output.log"))
    
    # hyperparameter ì €ì¥
    with open(os.path.join(work_dir, "settings.json"), "w", encoding="utf-8") as f:
        json.dump(CFG, f, indent=4, ensure_ascii=False)
    with open(os.path.join(work_dir, "CFG.py"), "w") as f:
        f.write("CFG = ")
        pprint.pprint(CFG, stream=f)

    # transform setting ì €ì¥
    save_transform(train_transform, os.path.join(work_dir, "train_transform.json"))
    save_transform(val_transform, os.path.join(work_dir, "val_transform.json"))
    
    # teacher model ì •ë³´ ê°€ì ¸ì˜¤ê¸°
    teacher_logits_dfs = []
    for logits_path in CFG['TEACHER_MODEL_LOGITS_CSVS']:
        teacher_logits_dfs.append(pd.read_csv(logits_path))
    
    print("Using device:", device)
    print(f"Using model: {CFG['MODEL_NAME']}")
    if CFG['RUN_SINGLE_FOLD']:
        print(f"Running SINGLE FOLD mode: Target Fold = {CFG['TARGET_FOLD']}/{CFG['N_FOLDS']}")
    else:
        print(f"Running ALL {CFG['N_FOLDS']} FOLDS mode.")
    print(f"Early Stopping Patience: {CFG['EARLY_STOPPING_PATIENCE']}")

    seed_everything(CFG['SEED'])

    train_root = CFG['ROOT'] # í•™ìŠµ ë°ì´í„° ê²½ë¡œ
    initial_dataset = InitialCustomImageDataset(train_root)
    if not initial_dataset.samples:
        raise ValueError(f"No images found in {train_root}. Please check the path and data structure.")
    print(f"ì´ í•™ìŠµ ì´ë¯¸ì§€ ìˆ˜ (K-Fold ëŒ€ìƒ): {len(initial_dataset.samples)}")

    all_samples = initial_dataset.samples
    targets = [s[1] for s in all_samples]
    class_names = initial_dataset.classes
    num_classes = len(class_names)
    print(f"í´ë˜ìŠ¤: {class_names} (ì´ {num_classes}ê°œ)")
    
    # ëª¨ë¸ì´ ì˜ëª» ë¶„ë¥˜í•œ ì˜ˆì‹œë¥¼ ì €ì¥í•˜ê¸° ìœ„í•œ í´ë” ìƒì„±
    wrong_save_path = os.path.join(work_dir, "wrong_examples")
    os.makedirs(wrong_save_path, exist_ok=True)


    # class_namesë¥¼ json íŒŒì¼ë¡œ ì €ì¥ (inf.pyì—ì„œ ì‚¬ìš©)
    with open(os.path.join(work_dir, 'class_names.json'), 'w') as f:
        json.dump(class_names, f)
    print(f"Saved class_names to class_names.json")

    skf = StratifiedKFold(n_splits=CFG['N_FOLDS'], shuffle=True, random_state=CFG['SEED'])
    overall_best_logloss = float('inf')
    overall_best_model_path = ""
    fold_results = []

    for fold_idx, (train_indices, val_indices) in enumerate(skf.split(all_samples, targets)):
        fold_num = fold_idx + 1
        if CFG['RUN_SINGLE_FOLD'] and fold_num != CFG['TARGET_FOLD']:
            print(f"\nSkipping Fold {fold_num}/{CFG['N_FOLDS']} as RUN_SINGLE_FOLD is True and TARGET_FOLD is {CFG['TARGET_FOLD']}.")
            fold_results.append({'fold': fold_num, 'best_logloss': None, 'model_path': None, 'status': 'skipped'})
            continue
        print(f"\n===== Running Fold {fold_num}/{CFG['N_FOLDS']} =====\n")

        train_samples_fold = [all_samples[i] for i in train_indices]
        val_samples_fold = [all_samples[i] for i in val_indices]
        train_dataset_fold = KnowledgeDistillationDataset(train_samples_fold, 
                                                          image_size = CFG['IMG_SIZE'], 
                                                          transform=train_transform, 
                                                          teacher_logits_list=teacher_logits_dfs, teacher_val_scores=CFG['TEACHER_VAL_LOSSES'])
        val_dataset_fold = FoldSpecificDataset(val_samples_fold, image_size = CFG['IMG_SIZE'], transform=val_transform, is_train=False)
        train_loader = DataLoader(train_dataset_fold, batch_size=CFG['BATCH_SIZE'], shuffle=True, num_workers=2, pin_memory=True)
        val_loader = DataLoader(val_dataset_fold, batch_size=CFG['BATCH_SIZE'], shuffle=False, num_workers=2, pin_memory=True)
        print(f"Fold {fold_num}: Train images: {len(train_dataset_fold)}, Validation images: {len(val_dataset_fold)}")

        student_model = CustomTimmModel(model_name=CFG['MODEL_NAME'], num_classes_to_predict=num_classes).to(device)
        model_path = CFG['START_FROM']
        if model_path and os.path.exists(model_path):
            student_model.load_state_dict(torch.load(model_path, map_location=device))
            print(f"{model_path} ëª¨ë¸ì„ ë¶ˆëŸ¬ì™€ í•´ë‹¹ ì²´í¬í¬ì¸íŠ¸ë¶€í„° í•™ìŠµì„ ì¬ê°œí•©ë‹ˆë‹¤. CFGë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
            print(f"Loaded model from {model_path}")
        else:
            print("ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œê°€ ì—†ê±°ë‚˜ ì œê³µë˜ì§€ ì•Šì•˜ìœ¼ë¯€ë¡œ pretrained modelìœ¼ë¡œë¶€í„° ëª¨ë¸ì„ í›ˆë ¨ì‹œí‚µë‹ˆë‹¤.")
        
        criterion = get_class_from_string(CFG['LOSS']['class'])(**CFG['LOSS']['params'])
        optimizer = get_class_from_string(CFG['OPTIMIZER']['class'])(student_model.parameters(), **CFG['OPTIMIZER']['params'])
        scheduler = get_class_from_string(CFG['SCHEDULER']['class'])(optimizer, **CFG['SCHEDULER']['params'])

        best_logloss_fold = float('inf')
        current_fold_best_model_path = None
        patience_counter = 0
        best_val_loss_for_early_stopping = float('inf')

        for epoch in range(CFG['EPOCHS']):
            student_model.train()
            train_loss_epoch = 0.0
            # tqdm ìƒëµ ê°€ëŠ¥ (ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ì‹œ) ë˜ëŠ” ìœ ì§€
            progress_bar = tqdm(train_loader, desc=f"[Fold {fold_num} Epoch {epoch+1}/{CFG['EPOCHS']}] Training", leave=False)
            for images, labels in progress_bar:
                images, labels, teacher_logits = images.to(device), labels.to(device), teacher_logits.to(device)
                optimizer.zero_grad()
                outputs = student_model(images)
                loss = loss_fn_kd(outputs, labels, teacher_logits, params=CFG['KD_PARAMS'])
                loss.backward()
                optimizer.step()
                train_loss_epoch += loss.item()
                progress_bar.set_postfix(total_loss=f"{loss.item():.4f}", lr=f"{optimizer.param_groups[0]['lr']:.1e}")
            avg_train_loss_epoch = train_loss_epoch / len(train_loader)

            student_model.eval()
            val_loss_epoch = 0.0
            correct_epoch = 0
            total_epoch = 0
            all_probs_epoch = []
            all_labels_epoch = []
            wrong_img_dict = defaultdict(list)
            with torch.no_grad():
                for images, labels, img_paths in tqdm(val_loader, desc=f"[Fold {fold_num} Epoch {epoch+1}/{CFG['EPOCHS']}] Validation", leave=False):
                    images, labels = images.to(device), labels.to(device)
                    outputs = student_model(images)
                    loss = criterion(outputs, labels)
                    val_loss_epoch += loss.item()
                    _, preds = torch.max(outputs, 1)
                    correct_epoch += (preds == labels).sum().item()
                    total_epoch += labels.size(0)
                    probs = F.softmax(outputs, dim=1)
                    all_probs_epoch.extend(probs.cpu().numpy())
                    all_labels_epoch.extend(labels.cpu().numpy())
                    
                    # === í‹€ë¦° ì˜ˆì¸¡ íƒìƒ‰ ===
                    correct_class_confidences = probs[torch.arange(len(labels)), labels]
                    wrong_indices = (correct_class_confidences <= CFG['WRONG_THRESHOLD']).nonzero(as_tuple=True)[0]  # í‹€ë¦° ì¸ë±ìŠ¤ë§Œ ì¶”ì¶œ
                    for idx in wrong_indices:
                        path = img_paths[idx]  # ì˜ˆ: 'data/train/cat/image1.jpg'
                        parent_folder = os.path.basename(os.path.dirname(path))  # ì˜ˆ: 'cat'
                        pred = preds[idx]
                        label = labels[idx]

                        if pred == label:
                            # ì˜ˆì¸¡ì€ ë§ì•˜ì§€ë§Œ confidenceê°€ ë‚®ìŒ â†’ ë‘ ë²ˆì§¸ë¡œ ë†’ì€ í´ë˜ìŠ¤ ì„ íƒ
                            sorted_probs, sorted_indices = probs[idx].sort(descending=True)
                            second_best_class = sorted_indices[1].item()
                            model_answer = class_names[second_best_class]
                        else:
                            # ì•„ì˜ˆ í‹€ë¦° ì˜ˆì¸¡ â†’ ê¸°ì¡´ëŒ€ë¡œ ì˜ˆì¸¡ ê²°ê³¼ ì‚¬ìš©
                            model_answer = class_names[pred]

                        wrong_img_dict[parent_folder].append({
                            'image_path': path,
                            'model_answer': model_answer
                        })
                with open(os.path.join(wrong_save_path, f"Fold_{fold_num}_Epoch_{epoch+1}_wrong_examples.json"), "w", encoding="utf-8") as f:
                    json.dump(wrong_img_dict, f, indent=4, ensure_ascii=False)
                    
            avg_val_loss_epoch = val_loss_epoch / len(val_loader) if len(val_loader) > 0 else float('inf')
            val_accuracy_epoch = 100 * correct_epoch / total_epoch if total_epoch > 0 else 0
            val_logloss_epoch = log_loss(all_labels_epoch, all_probs_epoch, labels=list(range(num_classes))) if total_epoch > 0 and len(np.unique(all_labels_epoch)) > 1 else float('inf')

            if CFG['SCHEDULER']['class'] == 'torch.optim.lr_scheduler.ReduceLROnPlateau':
                scheduler.step(val_logloss_epoch)
            else:
                scheduler.step()
            current_lr = optimizer.param_groups[0]['lr']
            print(f"Fold {fold_num} Epoch {epoch+1} - Train Loss: {avg_train_loss_epoch:.4f} | Valid Loss: {avg_val_loss_epoch:.4f} | Valid Acc: {val_accuracy_epoch:.2f}% | Valid LogLoss: {val_logloss_epoch:.4f} | LR: {current_lr:.1e}")

            if val_logloss_epoch < best_logloss_fold:
                best_logloss_fold = val_logloss_epoch
                current_fold_best_model_path = os.path.join(work_dir, f'best_model_{CFG["MODEL_NAME"]}_fold{fold_num}.pth')
                torch.save(student_model.state_dict(), current_fold_best_model_path)
                print(f"Fold {fold_num} ğŸ“¦ Best model saved at epoch {epoch+1} (LogLoss: {best_logloss_fold:.4f}) to {current_fold_best_model_path}")

            if val_logloss_epoch < best_val_loss_for_early_stopping:
                best_val_loss_for_early_stopping = val_logloss_epoch
                patience_counter = 0
            else:
                patience_counter += 1
            if patience_counter >= CFG['EARLY_STOPPING_PATIENCE']:
                print(f"Fold {fold_num} Early stopping triggered at epoch {epoch+1}.")
                break
        
        fold_results.append({'fold': fold_num, 'best_logloss': best_logloss_fold if best_logloss_fold != float('inf') else None, 'model_path': current_fold_best_model_path, 'status': 'completed'})
        if current_fold_best_model_path and best_logloss_fold < overall_best_logloss:
            overall_best_logloss = best_logloss_fold
            overall_best_model_path = current_fold_best_model_path
            print(f"ğŸŒŸ New Overall Best Model from Fold {fold_num} (LogLoss: {overall_best_logloss:.4f}, Path: {overall_best_model_path})")
        # ì „ì²´ í‹€ë¦° ê·¸ë£¹ì„ ì €ì¥
        get_total_wrong_groups(work_dir, CFG['GROUP_JSON_START_EPOCH'], fold_num)
        
    print("\n===== K-Fold Cross Validation Summary =====")
    # ... (ê²°ê³¼ ìš”ì•½ ë¶€ë¶„ì€ ë™ì¼í•˜ê²Œ ìœ ì§€)
    total_logloss_sum = 0
    valid_folds_count = 0
    executed_folds_count = 0
    for res in fold_results:
        if res['status'] == 'completed':
            executed_folds_count +=1
            print(f"Fold {res['fold']}: Best LogLoss = {res['best_logloss'] if res['best_logloss'] else 'N/A'}, Model Path = {res['model_path'] if res['model_path'] else 'N/A'} ({res['status']})")
            if res['best_logloss']:
                total_logloss_sum += res['best_logloss']
                valid_folds_count +=1
        elif res['status'] == 'skipped':
            print(f"Fold {res['fold']}: Status = {res['status']}")

    if executed_folds_count > 0 and valid_folds_count > 0:
        avg_logloss_executed_folds = total_logloss_sum / valid_folds_count
        print(f"\nAverage Best LogLoss across {valid_folds_count} successfully completed and valid folds: {avg_logloss_executed_folds:.4f}")
    elif executed_folds_count > 0:
        print("\nNo folds successfully saved a model to calculate average logloss.")
    else:
        print("\nNo folds were executed.")

    print(f"\nOverall Best LogLoss (among executed folds): {overall_best_logloss if overall_best_logloss != float('inf') else 'N/A'}")
    print(f"Path to the overall best model for inference: {overall_best_model_path if overall_best_model_path else 'N/A'}")
    print("Training finished.")

if __name__ == '__main__':
    train_main()