# train.pyí•˜ê³  í¬ê²Œ ë‹¤ë¥¼ ê±´ ì—†ìœ¼ë‚˜ Albumentationì„ ì ìš©í•´ë³´ê¸° ìœ„í•´ ìž‘ì„±í•œ .pyíŒŒì¼ìž…ë‹ˆë‹¤.

import os
import sys
import json
import random
import pandas as pd
import numpy as np
from PIL import Image
from tqdm import tqdm
import json # class_names ì €ìž¥ì„ ìœ„í•´ ì¶”ê°€
from sklearn.model_selection import StratifiedKFold
import torch
from torch.utils.data import Dataset, DataLoader
import timm
import torchvision.transforms as transforms
from torchvision.transforms import v2
import torch.nn.functional as F
from torch import nn, optim
from sklearn.metrics import log_loss
from collections import defaultdict
import albumentations as A
from albumentations.pytorch import ToTensorV2
import collections.abc  # for checking if transform is callable
from augmentations import *

# Device Setting
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Hyperparameter Setting
CFG = {
    "ROOT": '/home/sh/hecto/train',
    "WORK_DIR": '/home/sh/hecto/tjrgus5/work_dir/convnext1260_half_image_retraining_with_wrongs_2',
    "START_FROM": '/home/sh/hecto/tjrgus5/work_dir/convnext_mosaic_or_cutmix+mixup_test/best_model_convnext_base.fb_in22k_ft_in1k_384_fold1.pth', # ë§Œì•½ Noneì´ ì•„ë‹Œ .pthíŒŒì¼ ê²½ë¡œ ìž…ë ¥í•˜ë©´ í•´ë‹¹ checkpointë¥¼ loadí•´ì„œ ì‹œìž‘

    "CUTMIX": False,
    "MIXUP": False,
    "CUTOUT": False,

    # --- ìƒˆë¡œìš´ Mosaic ê´€ë ¨ ì„¤ì • ---
    'APPLY_MOSAIC_GROUP_P': 1, # Mosaic ê³„ì—´(Half ë˜ëŠ” Standard) ì¦ê°•ì„ ì ìš©í•  ì „ì²´ í™•ë¥ 
    'HALF_MOSAIC_ENABLED': True,    # Half-Mosaic ì‚¬ìš© ì—¬ë¶€ (ë§ˆìŠ¤í„° ìŠ¤ìœ„ì¹˜)
    'STANDARD_MOSAIC_ENABLED': False,  # Standard (4-cell) Mosaic ì‚¬ìš© ì—¬ë¶€ (ë§ˆìŠ¤í„° ìŠ¤ìœ„ì¹˜)
                                     # ê¸°ì¡´ HALF_MOSAIC_P, MOSAIC_PëŠ” ì´ ë¡œì§ì—ì„œ ì‚¬ìš©ë˜ì§€ ì•ŠìŒ
    # --------------------------------

    'IMG_SIZE': 512,
    'BATCH_SIZE': 32,
    'EPOCHS': 30,
    'LEARNING_RATE': 1e-5,
    'SEED' : 42,
    'MODEL_NAME': 'convnext_base.fb_in22k_ft_in1k_384',
    'N_FOLDS': 5,
    'EARLY_STOPPING_PATIENCE': 3,
    'RUN_SINGLE_FOLD': True,
    'TARGET_FOLD': 1
}

# 1) ë„¤ê°€ ì œê³µí•œ í˜¼ë™ í´ëž˜ìŠ¤ ìŒ ë¦¬ìŠ¤íŠ¸
confusion_pairs = [
    ("ì•„ë°˜ë–¼_í•˜ì´ë¸Œë¦¬ë“œ_CN7_2021_2023", "ì•„ë°˜ë–¼_CN7_2021_2023"),
    ("GLC_í´ëž˜ìŠ¤_X253_2020_2022", "GLC_í´ëž˜ìŠ¤_X253_2023"),
    ("K8_2022_2024", "K8_í•˜ì´ë¸Œë¦¬ë“œ_2022_2024"),
    ("íŠ¸ë ˆì¼ë¸”ë ˆì´ì €_2023", "íŠ¸ë ˆì¼ë¸”ë ˆì´ì €_2021_2022"),
    ("K7_í”„ë¦¬ë¯¸ì–´_í•˜ì´ë¸Œë¦¬ë“œ_2020_2021", "K7_í”„ë¦¬ë¯¸ì–´_2020_2021"),
    ("4ì‹œë¦¬ì¦ˆ_G22_2021_2023", "4ì‹œë¦¬ì¦ˆ_G22_2024_2025"),
    ("ë”_ë„¥ìŠ¤íŠ¸_ìŠ¤íŒŒí¬_2016_2018", "ë”_ë‰´_ìŠ¤íŒŒí¬_2019_2022"),
    ("ë”_ë‰´_K5_3ì„¸ëŒ€_2024_2025", "ë”_ë‰´_K5_í•˜ì´ë¸Œë¦¬ë“œ_3ì„¸ëŒ€_2023_2025"),
    ("ë ˆì¸ì§€ë¡œë²„_4ì„¸ëŒ€_2014_2017", "ë ˆì¸ì§€ë¡œë²„_4ì„¸ëŒ€_2018_2022"),
    ("5008_2ì„¸ëŒ€_2021_2024", "3008_2ì„¸ëŒ€_2018_2023"),
    ("3008_2ì„¸ëŒ€_2018_2023", "5008_2ì„¸ëŒ€_2018_2019"),
    ("7ì‹œë¦¬ì¦ˆ_G11_2016_2018", "7ì‹œë¦¬ì¦ˆ_G11_2019_2022"),
    ("EQE_V295_2022_2024", "EQS_V297_2022_2023"),
    ("K5_3ì„¸ëŒ€_í•˜ì´ë¸Œë¦¬ë“œ_2020_2022", "K5_3ì„¸ëŒ€_2020_2023"),
    ("ë¼ë¸Œ4_5ì„¸ëŒ€_2019_2024", "RAV4_5ì„¸ëŒ€_2019_2024"),
    ("ë ˆì¸ì§€ë¡œë²„_ì´ë³´í¬_2ì„¸ëŒ€_2023_2024", "ë ˆì¸ì§€ë¡œë²„_ì´ë³´í¬_2ì„¸ëŒ€_2020_2022"),
    ("ìŠ¤íŒ…ì–´_ë§ˆì´ìŠ¤í„°_2021_2023", "ìŠ¤íŒ…ì–´_2018_2020"),
    ("3ì‹œë¦¬ì¦ˆ_GT_F34_2014_2021", "4ì‹œë¦¬ì¦ˆ_F32_2014_2020"),
    ("GLE_í´ëž˜ìŠ¤_W166_2016_2018", "4ì‹œë¦¬ì¦ˆ_G22_2024_2025"),
    ("5008_2ì„¸ëŒ€_2018_2019", "5008_2ì„¸ëŒ€_2021_2024"),
    ("M5_F90_2018_2023", "5ì‹œë¦¬ì¦ˆ_G30_2017_2023"),
    ("ë”_ë‰´ìŠ¤í¬í‹°ì§€R_2014_2016", "5ì‹œë¦¬ì¦ˆ_G60_2024_2025"),
    ("6ì‹œë¦¬ì¦ˆ_GT_G32_2021_2024", "6ì‹œë¦¬ì¦ˆ_GT_G32_2018_2020"),
    ("ê·¸ëžœë“œì¹´ë‹ˆë°œ_2006_2010", "6ì‹œë¦¬ì¦ˆ_GT_G32_2018_2020"),
    ("ë°•ìŠ¤í„°_718_2017_2024", "GLE_í´ëž˜ìŠ¤_W167_2019_2024"),
    ("Q30_2017_2019", "911_992_2020_2024"),
    ("Q30_2017_2019", "G_í´ëž˜ìŠ¤_W463b_2019_2025"),
    ("ì œë„¤ì‹œìŠ¤_DH_2014_2016", "G80_2017_2020"),
    ("ì¹´ì´ì—”_PO536_2019_2023", "G_í´ëž˜ìŠ¤_W463b_2019_2025"),
    ("ë”_ì˜¬ë‰´G80_2021_2024", "K5_2ì„¸ëŒ€_2016_2018"),
    ("K5_3ì„¸ëŒ€_2020_2023", "K5_í•˜ì´ë¸Œë¦¬ë“œ_3ì„¸ëŒ€_2020_2023"),
    ("EQA_H243_2021_2024", "Q30_2017_2019"),
    ("ë‰´_ES300h_2013_2015", "Q50_2014_2017"),
    ("ìŠ¤í¬í‹°ì§€_4ì„¸ëŒ€_2016_2018", "Q5_FY_2021_2024"),
    ("7ì‹œë¦¬ì¦ˆ_F01_2009_2015", "Q7_4M_2020_2023"),
    ("7ì‹œë¦¬ì¦ˆ_F01_2009_2015", "X4_F26_2015_2018"),
    ("E_í´ëž˜ìŠ¤_W213_2017_2020", "S_í´ëž˜ìŠ¤_W222_2014_2020"),
    ("C_í´ëž˜ìŠ¤_W205_2015_2021", "S_í´ëž˜ìŠ¤_W222_2014_2020"),
    ("G70_2018_2020", "S_í´ëž˜ìŠ¤_W223_2021_2025"),
    ("ë”_ë‰´_G70_2021_2025", "X1_F48_2020_2022"),
    ("X3_G01_2022_2024", "X4_G02_2022_2025"),
    ("X6_G06_2020_2023", "X6_G06_2024_2025"),
    ("XC90_2ì„¸ëŒ€_2020_2025", "XC90_2ì„¸ëŒ€_2017_2019"),
    ("XM3_2024", "XM3_2020_2023"),
    ("RAV4_5ì„¸ëŒ€_2019_2024", "ê·¸ëžœë“œ_ì²´ë¡œí‚¤_WL_2021_2023"),
    ("ë‰´_A6_2012_2014", "ë‰´_A6_2015_2018"),
    ("ë‰´_G80_2025_2026", "ë‰´_GV80_2024_2025"),
    ("ê·¸ëžœë“œì¹´ë‹ˆë°œ_2006_2010", "ë‰´_SM5_ìž„í”„ë ˆì…˜_2008_2010"),
    ("ë‰´_QM6_2021_2023", "ë”_ë‰´_QM6_2020_2023"),
    ("SM6_2016_2020", "ë”_ë‰´_SM6_2021_2024"),
    ("F150_2004_2021", "ë”_ë‰´_ê·¸ëžœë“œ_ìŠ¤íƒ€ë ‰ìŠ¤_2018_2021"),
    ("ê·¸ëžœë“œ_ìŠ¤íƒ€ë ‰ìŠ¤_2016_2018", "ë”_ë‰´_ê·¸ëžœë“œ_ìŠ¤íƒ€ë ‰ìŠ¤_2018_2021"),
    ("ë ‰ìŠ¤í„´_ìŠ¤í¬ì¸ _ì¹¸_2019_2020", "ë”_ë‰´_ë ‰ìŠ¤í„´_ìŠ¤í¬ì¸ _ì¹¸_2021_2025"),
    ("íŒŒë‚˜ë©”ë¼_2010_2016", "ë”_ë‰´_ì•„ë°˜ë–¼_2014_2016"),
    ("ì˜¬_ë‰´_ì¹´ë‹ˆë°œ_2015_2019", "ë”_ë‰´_ì¹´ë‹ˆë°œ_2019_2020"),
    ("íˆ¬ì‹¼_NX4_2021_2023", "ë”_ë‰´_íˆ¬ì‹¼_NX4_2023_2025"),
    ("ê¸€ëž˜ë””ì—ì´í„°_JT_2020_2023", "ëž­ê¸€ëŸ¬_JL_2018_2024"),
    ("ë ˆë‹ˆê²Œì´ë“œ_2015_2017", "ë ˆë‹ˆê²Œì´ë“œ_2019_2023"),
    ("XJ_8ì„¸ëŒ€_2010_2019", "ë¨¸ìŠ¤íƒ±_2015_2023"),
    ("ê·¸ëžœì €_HG_2011_2014", "ë°•ìŠ¤í„°_718_2017_2024"),
    ("ë””_ì˜¬ë‰´ì‹¼íƒ€íŽ˜_2024_2025", "ì‹¼íƒ€íŽ˜_MX5_2024_2025"),
    ("SM7_ë‰´ì•„íŠ¸_2008_2011", "ì•„ë² ì˜¤_2012_2016"),
    ("ì˜¬_ë‰´_K7_2016_2019", "ì˜¬_ë‰´_K7_í•˜ì´ë¸Œë¦¬ë“œ_2017_2019"),
    ("ì¹´ë‹ˆë°œ_4ì„¸ëŒ€_2021", "ì¹´ë‹ˆë°œ_4ì„¸ëŒ€_2022_2023"),
    ("CLS_í´ëž˜ìŠ¤_C257_2019_2023", "ì»¨í‹°ë„¨íƒˆ_GT_3ì„¸ëŒ€_2018_2023"),
    ("ë¦¬ì–¼_ë‰´_ì½œë¡œë¼ë„_2021_2022", "ì½œë¡œë¼ë„_2020_2020"),
    ("í‹°ë³¼ë¦¬_ì•„ë¨¸_2018_2019", "í‹°ë³¼ë¦¬_2015_2018"),
    ("íŒŒë‚˜ë©”ë¼_971_2017_2023", "íŒŒë‚˜ë©”ë¼_2010_2016"),
    ("All_New_XJ_2016_2019", "XJ_8ì„¸ëŒ€_2010_2019"),
    ("ê·¸ëžœì €_HG_2015_2017", "ê·¸ëžœì €_HG_2011_2014"),
    ("ì•„ë°˜ë–¼_CN7_2021_2023", "ë”_ë‰´_ì•„ë°˜ë–¼_CN7_2023_2025")
]

# 2) ì–‘ë°©í–¥ similarity_map ìƒì„± (ì¤‘ë³µì€ set ìœ¼ë¡œ ê±¸ëŸ¬ì¤ë‹ˆë‹¤)
from collections import defaultdict

similarity_map = defaultdict(set)
for a, b in confusion_pairs:
    similarity_map[a].add(b)
    similarity_map[b].add(a)

# 3) ìµœì¢… dict í˜•íƒœë¡œ ë³€í™˜
similarity_map = {k: list(v) for k, v in similarity_map.items()}

class Logger:
    def __init__(self, filepath):
        self.terminal = sys.stdout
        self.log = open(filepath, 'w')

    def write(self, message):
        self.terminal.write(message)  # í„°ë¯¸ë„ì—ë„ ì¶œë ¥
        self.log.write(message)       # íŒŒì¼ì—ë„ ê¸°ë¡

    def flush(self):
        self.terminal.flush()
        self.log.flush()

# Fixed RandomSeed
def seed_everything(seed):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False


# --- InitialCustomImageDataset (ì´ˆê¸° ë°ì´í„° ë¡œë“œìš©) ---
class InitialCustomImageDataset(Dataset):
    def __init__(self, root_dir):
        self.root_dir = root_dir
        self.samples = []
        self.classes = sorted([d for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, d))])
        if not self.classes:
            raise ValueError(f"No class subdirectories found in {root_dir}")
        self.class_to_idx = {cls_name: i for i, cls_name in enumerate(self.classes)}

        for cls_name in self.classes:
            cls_folder = os.path.join(root_dir, cls_name)
            for fname in os.listdir(cls_folder):
                if fname.lower().endswith(('.jpg', '.jpeg', '.png')):
                    img_path = os.path.join(cls_folder, fname)
                    label = self.class_to_idx[cls_name]
                    self.samples.append((img_path, label))
    def __len__(self):
        return len(self.samples)
    def __getitem__(self, idx):
        return self.samples[idx]
        
class FoldSpecificDataset(Dataset):
    def __init__(self, samples_list, transform=None, is_train=True):
        self.samples_list = samples_list
        self.transform = transform
        self.is_train = is_train

    def __len__(self):
        return len(self.samples_list)

    def __getitem__(self, idx):
        img_path, label = self.samples_list[idx]
        try:
            image = Image.open(img_path).convert('RGB')
        except FileNotFoundError:
            dummy_tensor = torch.zeros((3, CFG['IMG_SIZE'], CFG['IMG_SIZE']))
            return (dummy_tensor, 0) if self.is_train else (dummy_tensor, 0, img_path)
        
        # transformì´ Albumentationsì¸ì§€ torchvisionì¸ì§€ êµ¬ë¶„
        if isinstance(self.transform, A.BasicTransform) or isinstance(self.transform, A.Compose):
            image = np.array(image)
            image = self.transform(image=image)['image']
        elif isinstance(self.transform, collections.abc.Callable):
            image = self.transform(image)

        return (image, label) if self.is_train else (image, label, img_path)


# --- í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ (inf.pyì—ì„œë„ ì‚¬ìš©) ---
class TestCustomImageDataset(Dataset):
    def __init__(self, root_dir, transform=None):
        self.root_dir = root_dir
        self.transform = transform
        self.samples = []
        if not os.path.exists(root_dir) or not os.path.isdir(root_dir):
            print(f"Warning: Test directory {root_dir} not found or is not a directory.")
            return
        for fname in sorted(os.listdir(root_dir)):
            if fname.lower().endswith(('.jpg', '.jpeg', '.png')):
                img_path = os.path.join(root_dir, fname)
                self.samples.append((img_path,))
    def __len__(self):
        return len(self.samples)
    
    def __getitem__(self, idx):
        img_path = self.samples[idx][0]
        try:
            image = Image.open(img_path).convert('RGB')
        except FileNotFoundError:
            # print(f"Warning: File not found {img_path}, returning a dummy image.")
            return torch.zeros((3, CFG['IMG_SIZE'], CFG['IMG_SIZE']))
        if self.transform:
            image = self.transform(image)
        return image

# Model Define (inf.pyì—ì„œë„ ì‚¬ìš©)
class CustomTimmModel(nn.Module):
    def __init__(self, model_name, num_classes_to_predict, pretrained=True):
        super(CustomTimmModel, self).__init__()
        try:
            self.backbone = timm.create_model(model_name, pretrained=pretrained, num_classes=0, global_pool='avg')
            self.feature_dim = self.backbone.num_features
        except Exception as e:
            print(f"Error creating model {model_name} with timm. Error: {e}")
            raise
        self.head = nn.Linear(self.feature_dim, num_classes_to_predict)
    def forward(self, x):
        features = self.backbone(x)
        output = self.head(features)
        return output

# --- Albumentations ê¸°ë°˜ ì´ë¯¸ì§€ ë³€í™˜ ì •ì˜ ---
train_transform = A.Compose([
    A.Resize(CFG['IMG_SIZE'], CFG['IMG_SIZE']),
    A.HorizontalFlip(p=0.5),
    A.Rotate(limit=15, p=0.5),
    A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1, p=0.5),
    A.Affine(translate_percent=(0.1, 0.1), scale=(0.9, 1.1), shear=10, rotate=0, p=0.5),
    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),
    ToTensorV2()
])

val_transform = A.Compose([
    A.Resize(CFG['IMG_SIZE'], CFG['IMG_SIZE']),
    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),
    ToTensorV2()
])


def train_main():
    # work directory ìƒì„±
    work_dir = CFG['WORK_DIR']
    os.makedirs(work_dir, exist_ok=True)
    
    # logger
    sys.stdout = Logger(os.path.join(work_dir, "output.log"))
    
    # hyperparameter ì €ìž¥
    with open(os.path.join(work_dir, "settings.json"), "w", encoding="utf-8") as f:
        json.dump(CFG, f, indent=4, ensure_ascii=False)
    
    print("Using device:", device)
    print(f"Using model: {CFG['MODEL_NAME']}")
    if CFG['RUN_SINGLE_FOLD']:
        print(f"Running SINGLE FOLD mode: Target Fold = {CFG['TARGET_FOLD']}/{CFG['N_FOLDS']}")
    else:
        print(f"Running ALL {CFG['N_FOLDS']} FOLDS mode.")
    print(f"Early Stopping Patience: {CFG['EARLY_STOPPING_PATIENCE']}")

    seed_everything(CFG['SEED'])

    train_root = CFG['ROOT'] # í•™ìŠµ ë°ì´í„° ê²½ë¡œ
    initial_dataset = InitialCustomImageDataset(train_root)
    if not initial_dataset.samples:
        raise ValueError(f"No images found in {train_root}. Please check the path and data structure.")
    print(f"ì´ í•™ìŠµ ì´ë¯¸ì§€ ìˆ˜ (K-Fold ëŒ€ìƒ): {len(initial_dataset.samples)}")

    all_samples = initial_dataset.samples
    targets = [s[1] for s in all_samples]
    class_names = initial_dataset.classes
    num_classes = len(class_names)
    print(f"í´ëž˜ìŠ¤: {class_names} (ì´ {num_classes}ê°œ)")

    # cutmix or mixup transform settings
    if CFG['CUTMIX'] and CFG["MIXUP"]:
        cutmix = v2.CutMix(num_classes=num_classes)
        mixup = v2.MixUp(num_classes=num_classes)
        cutmix_or_mixup = v2.RandomChoice([cutmix, mixup])
        print("ë§¤ ë°°ì¹˜ë§ˆë‹¤ CUTMIXì™€ MIXUPì„ ëžœë¤í•˜ê²Œ ì ìš©í•©ë‹ˆë‹¤. CFGë¥¼ í™•ì¸í•˜ì„¸ìš”.")
    elif CFG['CUTMIX']:
        cutmix_or_mixup = v2.CutMix(num_classes=num_classes)
        print("ë§¤ ë°°ì¹˜ë§ˆë‹¤ CUTMIXë¥¼ ëžœë¤í•˜ê²Œ ì ìš©í•©ë‹ˆë‹¤. CFGë¥¼ í™•ì¸í•˜ì„¸ìš”.")
    elif CFG['MIXUP']:
        cutmix_or_mixup = v2.MixUp(num_classes=num_classes)
        print("ë§¤ ë°°ì¹˜ë§ˆë‹¤ MIXUPì„ ëžœë¤í•˜ê²Œ ì ìš©í•©ë‹ˆë‹¤. CFGë¥¼ í™•ì¸í•˜ì„¸ìš”.")
    else:
        cutmix_or_mixup = None
    
    # ëª¨ë¸ì´ ìž˜ëª» ë¶„ë¥˜í•œ ì˜ˆì‹œë¥¼ ì €ìž¥í•˜ê¸° ìœ„í•œ í´ë” ìƒì„±
    wrong_save_path = os.path.join(work_dir, "wrong_examples")
    os.makedirs(wrong_save_path, exist_ok=True)


    # class_namesë¥¼ json íŒŒì¼ë¡œ ì €ìž¥ (inf.pyì—ì„œ ì‚¬ìš©)
    with open(os.path.join(work_dir, 'class_names.json'), 'w') as f:
        json.dump(class_names, f)
    print(f"Saved class_names to class_names.json")


    skf = StratifiedKFold(n_splits=CFG['N_FOLDS'], shuffle=True, random_state=CFG['SEED'])
    overall_best_logloss = float('inf')
    overall_best_model_path = ""
    fold_results = []

    for fold_idx, (train_indices, val_indices) in enumerate(skf.split(all_samples, targets)):
        fold_num = fold_idx + 1
        if CFG['RUN_SINGLE_FOLD'] and fold_num != CFG['TARGET_FOLD']:
            print(f"\nSkipping Fold {fold_num}/{CFG['N_FOLDS']} as RUN_SINGLE_FOLD is True and TARGET_FOLD is {CFG['TARGET_FOLD']}.")
            fold_results.append({'fold': fold_num, 'best_logloss': None, 'model_path': None, 'status': 'skipped'})
            continue
        print(f"\n===== Running Fold {fold_num}/{CFG['N_FOLDS']} =====\n")

        train_samples_fold = [all_samples[i] for i in train_indices]
        val_samples_fold = [all_samples[i] for i in val_indices]
        train_dataset_fold = FoldSpecificDataset(train_samples_fold, transform=train_transform)
        val_dataset_fold = FoldSpecificDataset(val_samples_fold, transform=val_transform, is_train=False)
        train_loader = DataLoader(train_dataset_fold, batch_size=CFG['BATCH_SIZE'], shuffle=True, num_workers=2, pin_memory=True)
        val_loader = DataLoader(val_dataset_fold, batch_size=CFG['BATCH_SIZE'], shuffle=False, num_workers=2, pin_memory=True)
        print(f"Fold {fold_num}: Train images: {len(train_dataset_fold)}, Validation images: {len(val_dataset_fold)}")

        model = CustomTimmModel(model_name=CFG['MODEL_NAME'], num_classes_to_predict=num_classes).to(device)
        model_path = CFG['START_FROM']
        if model_path and os.path.exists(model_path):
            model.load_state_dict(torch.load(model_path, map_location=device))
            print(f"{model_path} ëª¨ë¸ì„ ë¶ˆëŸ¬ì™€ í•´ë‹¹ ì²´í¬í¬ì¸íŠ¸ë¶€í„° í•™ìŠµì„ ìž¬ê°œí•©ë‹ˆë‹¤. CFGë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
            print(f"Loaded model from {model_path}")
        else:
            print("ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œê°€ ì—†ê±°ë‚˜ ì œê³µë˜ì§€ ì•Šì•˜ìœ¼ë¯€ë¡œ pretrained modelìœ¼ë¡œë¶€í„° ëª¨ë¸ì„ í›ˆë ¨ì‹œí‚µë‹ˆë‹¤.")
        
        criterion = nn.CrossEntropyLoss()
        optimizer = optim.AdamW(model.parameters(), lr=CFG['LEARNING_RATE'], weight_decay=1e-2)
        # scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2, verbose=False)
        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=14, gamma=0.1)

        best_logloss_fold = float('inf')
        current_fold_best_model_path = None
        patience_counter = 0
        best_val_loss_for_early_stopping = float('inf')

        for epoch in range(CFG['EPOCHS']):
            model.train()
            train_loss_epoch = 0.0
            # tqdm ìƒëžµ ê°€ëŠ¥ (ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ì‹œ) ë˜ëŠ” ìœ ì§€
            for images, labels in tqdm(train_loader, desc=f"[Fold {fold_num} Epoch {epoch+1}/{CFG['EPOCHS']}] Training", leave=False):
                images, labels = images.to(device), labels.to(device)

                # cutoutì„ ìœ„í•´ ì¶”ê°€
                if CFG['CUTOUT']:
                    images = apply_cutout(images, mask_size = 64)
                
                # cutmix mixupì„ ìœ„í•´ ì¶”ê°€
                if cutmix_or_mixup:
                    images, labels = cutmix_or_mixup(images, labels)
                
    # --- ìˆ˜ì •ëœ Mosaic ê³„ì—´ ì¦ê°• ë¡œì§ ---
                applied_special_mosaic = False # ì´ë²ˆ ë°°ì¹˜ì— Half ë˜ëŠ” Standard Mosaicì´ ì ìš©ë˜ì—ˆëŠ”ì§€ ì¶”ì 

                # 1. Mosaic ê³„ì—´ ì¦ê°•ì„ ì ìš©í• ì§€ ì „ì²´ í™•ë¥ (APPLY_MOSAIC_GROUP_P)ë¡œ ê²°ì •
                if CFG.get('APPLY_MOSAIC_GROUP_P', 0.0) > 0 and \
                random.random() < CFG.get('APPLY_MOSAIC_GROUP_P'):
                    
                    can_apply_half_mosaic = CFG.get('HALF_MOSAIC_ENABLED', False)
                    can_apply_standard_mosaic = CFG.get('STANDARD_MOSAIC_ENABLED', False)

                    # 2. ì–´ë–¤ Mosaicì„ ì ìš©í• ì§€ ê²°ì •
                    if can_apply_half_mosaic and can_apply_standard_mosaic:
                        # Half-Mosaicê³¼ Standard Mosaic ëª¨ë‘ í™œì„±í™”ëœ ê²½ìš°, 50:50 í™•ë¥ ë¡œ ì„ íƒ
                        if random.random() < 0.5: 
                            orientation = random.choice(['horizontal', 'vertical'])
                            images, labels = half_mosaic( # ì‚¬ìš©ìž ì •ì˜ half_mosaic í˜¸ì¶œ
                                images, labels,
                                class_names, similarity_map, 
                                num_classes,
                                orientation=orientation
                            )
                            applied_special_mosaic = True
                        else: 
                            images, labels = mosaic_augmentation( # ì‚¬ìš©ìž ì •ì˜ mosaic_augmentation í˜¸ì¶œ
                                images, labels, num_classes # train_mainì—ì„œ ì •ì˜ëœ num_classes ì‚¬ìš©
                            )
                            applied_special_mosaic = True
                    elif can_apply_half_mosaic: # Half-Mosaicë§Œ í™œì„±í™”ëœ ê²½ìš°
                        orientation = random.choice(['horizontal', 'vertical'])
                        images, labels = half_mosaic(
                            images, labels,
                            class_names, similarity_map,
                            num_classes,
                            orientation=orientation
                        )
                        applied_special_mosaic = True
                    elif can_apply_standard_mosaic: # Standard Mosaicë§Œ í™œì„±í™”ëœ ê²½ìš°
                        images, labels = mosaic_augmentation(
                            images, labels, num_classes
                        )
                        applied_special_mosaic = True

                optimizer.zero_grad()
                outputs = model(images)
                loss = criterion(outputs, labels)
                loss.backward()
                optimizer.step()
                train_loss_epoch += loss.item()
            avg_train_loss_epoch = train_loss_epoch / len(train_loader)

            model.eval()
            val_loss_epoch = 0.0
            correct_epoch = 0
            total_epoch = 0
            all_probs_epoch = []
            all_labels_epoch = []
            wrong_img_dict = defaultdict(list)
            with torch.no_grad():
                for images, labels, img_paths in tqdm(val_loader, desc=f"[Fold {fold_num} Epoch {epoch+1}/{CFG['EPOCHS']}] Validation", leave=False):
                    images, labels = images.to(device), labels.to(device)
                    outputs = model(images)
                    loss = criterion(outputs, labels)
                    val_loss_epoch += loss.item()
                    _, preds = torch.max(outputs, 1)
                    correct_epoch += (preds == labels).sum().item()
                    total_epoch += labels.size(0)
                    probs = F.softmax(outputs, dim=1)
                    all_probs_epoch.extend(probs.cpu().numpy())
                    all_labels_epoch.extend(labels.cpu().numpy())
                    
                    # === í‹€ë¦° ì˜ˆì¸¡ íƒìƒ‰ ===
                    wrong_indices = (preds != labels).nonzero(as_tuple=True)[0]
                    for i in wrong_indices: # ì¸ë±ìŠ¤ ë³€ìˆ˜ëª… ë³€ê²½ (ì„ íƒ ì‚¬í•­)
                        true_label_idx = labels[i].item()
                        predicted_label_idx = preds[i].item()

                        image_path_for_wrong = img_paths[i]
                        true_class_name_for_wrong = class_names[true_label_idx]
                        predicted_class_name_for_wrong = class_names[predicted_label_idx]

                        # JSON í‚¤ë¥¼ ëª¨ë¸ì´ ì˜ˆì¸¡í•œ í´ëž˜ìŠ¤ëª…ìœ¼ë¡œ ë³€ê²½
                        wrong_img_dict[predicted_class_name_for_wrong].append({
                            'image_path': image_path_for_wrong,
                            'correct_answer': true_class_name_for_wrong # í•„ë“œëª…ë„ ì´ì „ê³¼ ë™ì¼í•˜ê²Œ
                        })
                with open(os.path.join(wrong_save_path, f"Fold_{fold_num}_Epoch_{epoch+1}_wrong_examples.json"), "w", encoding="utf-8") as f:
                    json.dump(wrong_img_dict, f, indent=4, ensure_ascii=False)
                    
            avg_val_loss_epoch = val_loss_epoch / len(val_loader) if len(val_loader) > 0 else float('inf')
            val_accuracy_epoch = 100 * correct_epoch / total_epoch if total_epoch > 0 else 0
            val_logloss_epoch = log_loss(all_labels_epoch, all_probs_epoch, labels=list(range(num_classes))) if total_epoch > 0 and len(np.unique(all_labels_epoch)) > 1 else float('inf')

            current_lr = optimizer.param_groups[0]['lr']
            print(f"Fold {fold_num} Epoch {epoch+1} - Train Loss: {avg_train_loss_epoch:.4f} | Valid Loss: {avg_val_loss_epoch:.4f} | Valid Acc: {val_accuracy_epoch:.2f}% | Valid LogLoss: {val_logloss_epoch:.4f} | LR: {current_lr:.1e}")
            scheduler.step()

            if val_logloss_epoch < best_logloss_fold:
                best_logloss_fold = val_logloss_epoch
                current_fold_best_model_path = os.path.join(work_dir, f'best_model_{CFG["MODEL_NAME"]}_fold{fold_num}.pth')
                torch.save(model.state_dict(), current_fold_best_model_path)
                print(f"Fold {fold_num} ðŸ“¦ Best model saved at epoch {epoch+1} (LogLoss: {best_logloss_fold:.4f}) to {current_fold_best_model_path}")

            if val_logloss_epoch < best_val_loss_for_early_stopping:
                best_val_loss_for_early_stopping = val_logloss_epoch
                patience_counter = 0
            else:
                patience_counter += 1
            if patience_counter >= CFG['EARLY_STOPPING_PATIENCE']:
                print(f"Fold {fold_num} Early stopping triggered at epoch {epoch+1}.")
                break
        
        fold_results.append({'fold': fold_num, 'best_logloss': best_logloss_fold if best_logloss_fold != float('inf') else None, 'model_path': current_fold_best_model_path, 'status': 'completed'})
        if current_fold_best_model_path and best_logloss_fold < overall_best_logloss:
            overall_best_logloss = best_logloss_fold
            overall_best_model_path = current_fold_best_model_path
            print(f"ðŸŒŸ New Overall Best Model from Fold {fold_num} (LogLoss: {overall_best_logloss:.4f}, Path: {overall_best_model_path})")

    print("\n===== K-Fold Cross Validation Summary =====")
    # ... (ê²°ê³¼ ìš”ì•½ ë¶€ë¶„ì€ ë™ì¼í•˜ê²Œ ìœ ì§€)
    total_logloss_sum = 0
    valid_folds_count = 0
    executed_folds_count = 0
    for res in fold_results:
        if res['status'] == 'completed':
            executed_folds_count +=1
            print(f"Fold {res['fold']}: Best LogLoss = {res['best_logloss']:.4f if res['best_logloss'] else 'N/A'}, Model Path = {res['model_path'] if res['model_path'] else 'N/A'} ({res['status']})")
            if res['best_logloss']:
                total_logloss_sum += res['best_logloss']
                valid_folds_count +=1
        elif res['status'] == 'skipped':
            print(f"Fold {res['fold']}: Status = {res['status']}")

    if executed_folds_count > 0 and valid_folds_count > 0:
        avg_logloss_executed_folds = total_logloss_sum / valid_folds_count
        print(f"\nAverage Best LogLoss across {valid_folds_count} successfully completed and valid folds: {avg_logloss_executed_folds:.4f}")
    elif executed_folds_count > 0:
        print("\nNo folds successfully saved a model to calculate average logloss.")
    else:
        print("\nNo folds were executed.")

    print(f"\nOverall Best LogLoss (among executed folds): {overall_best_logloss:.4f if overall_best_logloss != float('inf') else 'N/A'}")
    print(f"Path to the overall best model for inference: {overall_best_model_path if overall_best_model_path else 'N/A'}")
    print("Training finished.")


if __name__ == '__main__':
    train_main()