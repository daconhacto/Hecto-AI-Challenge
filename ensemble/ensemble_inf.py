import os
import pandas as pd
import numpy as np
import json
import torch
from torch.utils.data import Dataset, DataLoader
from tqdm import tqdm
import torchvision.transforms as transforms
import torch.nn.functional as F
from PIL import Image # TestCustomImageDatasetì—ì„œ ì‚¬ìš©ë  ìˆ˜ ìˆìŒ

# train.pyì—ì„œ í•„ìš”í•œ í´ë˜ìŠ¤ ë° í•¨ìˆ˜ë¥¼ ê°€ì ¸ì˜¨ë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤.
# ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” train.pyì˜ ì •í™•í•œ ê²½ë¡œ ë° ë‚´ìš©ì„ í™•ì¸í•´ì•¼ í•©ë‹ˆë‹¤.
# ì—¬ê¸°ì„œëŠ” CustomTimmModel, TestCustomImageDataset, seed_everything, TRAIN_CFGê°€ ìˆë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤.
# ë§Œì•½ train.pyì— í•´ë‹¹ ì •ì˜ê°€ ì—†ë‹¤ë©´, ì´ ìŠ¤í¬ë¦½íŠ¸ ë‚´ì— ì§ì ‘ ì •ì˜í•˜ê±°ë‚˜ ì˜¬ë°”ë¥´ê²Œ import í•´ì•¼ í•©ë‹ˆë‹¤.

# ---- train.pyë¡œë¶€í„° ê°€ì ¸ì™€ì•¼ í•  (ë˜ëŠ” ì´ íŒŒì¼ì— ì •ì˜í•´ì•¼ í• ) ìš”ì†Œë“¤ ----
# ì˜ˆì‹œ:
class CustomTimmModel(torch.nn.Module): # train.pyì˜ CustomTimmModel ì •ì˜ì™€ ë™ì¼í•´ì•¼ í•¨
    def __init__(self, model_name, num_classes_to_predict, pretrained=True):
        super(CustomTimmModel, self).__init__()
        self.backbone = timm.create_model(model_name, pretrained=pretrained, num_classes=0, global_pool='avg')
        self.feature_dim = self.backbone.num_features
        self.head = torch.nn.Linear(self.feature_dim, num_classes_to_predict)
    def forward(self, x):
        features = self.backbone(x)
        output = self.head(features)
        return output

class TestCustomImageDataset(Dataset): # train.pyì˜ TestCustomImageDataset ì •ì˜ì™€ ë™ì¼í•´ì•¼ í•¨
    def __init__(self, root_dir, transform=None):
        self.root_dir = root_dir
        self.transform = transform
        self.samples = []
        if not os.path.exists(root_dir) or not os.path.isdir(root_dir):
            print(f"Warning: Test directory {root_dir} not found or is not a directory.")
            return
        for fname in sorted(os.listdir(root_dir)):
            if fname.lower().endswith(('.jpg', '.jpeg', '.png')):
                img_path = os.path.join(root_dir, fname)
                self.samples.append((img_path,))
    def __len__(self):
        return len(self.samples)
    def __getitem__(self, idx):
        img_path = self.samples[idx][0]
        try:
            image = Image.open(img_path).convert('RGB')
        except FileNotFoundError:
            # ì˜ˆì‹œ TRAIN_CFGê°€ ì—†ìœ¼ë¯€ë¡œ ê¸°ë³¸ê°’ ì‚¬ìš© ë˜ëŠ” CFG_ENSì—ì„œ IMG_SIZE ê°€ì ¸ì˜¤ë„ë¡ ìˆ˜ì • í•„ìš”
            return torch.zeros((3, CFG_ENS.get('DEFAULT_IMG_SIZE', 512), CFG_ENS.get('DEFAULT_IMG_SIZE', 512)))
        if self.transform:
            image = self.transform(image)
        return image

def seed_everything(seed): # train.pyì˜ seed_everything ì •ì˜ì™€ ë™ì¼í•´ì•¼ í•¨
    import random
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

# timm ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤. pip install timm
import timm
# -----------------------------------------------------------------------

# í˜„ì¬ ìŠ¤í¬ë¦½íŠ¸ìš© ì„¤ì •
CFG_ENS = {
    'MODELS_TO_ENSEMBLE': [
        # ì¤‘ìš”: ì•„ë˜ 'val_log_loss'ëŠ” ê° ëª¨ë¸ì˜ ì‹¤ì œ ê²€ì¦ Log Loss ê°’ìœ¼ë¡œ ì±„ì›Œì•¼ í•©ë‹ˆë‹¤!
       # {'path': '/data2/project/2025winter/tjrgus357/hacto/check_point/1114.pth', 'name': 'convnext_base.fb_in22k_ft_in1k_384', 'img_size': 640, 'val_log_loss': 0.1114},
        #{'path': '/data2/project/2025winter/tjrgus357/hacto/check_point/EVA_large(12139)_fold1.pth', 'name': 'eva02_large_patch14_448.mim_m38m_ft_in1k', 'img_size': 448, 'val_log_loss': 0.12139},
        # {'path': '/home/tjrgus357/hecto_original/check_point/Salinecy(13354)_fold1.pth', 'name': 'convnext_base.fb_in22k_ft_in1k_384', 'img_size': 512, 'val_log_loss': 0.13354}, 
        {'path': '/data2/project/2025winter/tjrgus357/hacto/check_point/1089.pth', 'name': 'convnext_base.fb_in22k_ft_in1k_384', 'img_size': 640, 'val_log_loss': 0.1089}, 


        {'path': '/data2/project/2025winter/tjrgus357/hacto/check_point/conv_large(0.09476).pth', 'name': 'convnext_large_mlp.clip_laion2b_augreg_ft_in1k_384', 'img_size': 600, 'val_log_loss': 0.09476},
        #{'path': '/home/tjrgus357/hecto_original/check_point/EVA_large(12139)_fold1.pth', 'name': 'eva02_large_patch14_448.mim_m38m_ft_in1k', 'img_size': 448, 'val_log_loss': 0.12139},
        #{'path': '/home/tjrgus357/hecto_original/check_point/Salinecy(13354)_fold1.pth', 'name': 'convnext_base.fb_in22k_ft_in1k_384', 'img_size': 512, 'val_log_loss': 0.13354}, 

        #{'path': '/home/tjrgus357/hecto_original/check_point/convnext(1218)_fold1.pth', 'name': 'convnext_base.fb_in22k_ft_in1k_384', 'img_size': 512, 'val_log_loss': 0.12188},
        # {'path': '/home/tjrgus357/hecto_original/check_point/EVA_large(12139)_fold1.pth', 'name': 'eva02_large_patch14_448.mim_m38m_ft_in1k', 'img_size': 448, 'val_log_loss': 0.12139},
        # {'path': '/data2/project/2025winter/tjrgus357/hacto/check_point/img_size_640(11684).pth', 'name': 'convnext_base.fb_in22k_ft_in1k_384', 'img_size': 640, 'val_log_loss': 0.11684}, 
 
    ],
    'BATCH_SIZE': 16, # ì¶”ë¡  ì‹œ ë°°ì¹˜ í¬ê¸° ì¡°ì ˆ ê°€ëŠ¥
    'SEED': 42,
    'CLASS_NAMES_PATH': './class_names.json', # train.py ì‹¤í–‰ í›„ ìƒì„±ëœ class_names.json ê²½ë¡œ
    'TEST_ROOT': '../test',
    'SAMPLE_SUBMISSION_PATH': '../sample_submission.csv',
    'DEFAULT_IMG_SIZE': 512 # TestCustomImageDatasetì˜ ë”ë¯¸ ì´ë¯¸ì§€ ìƒì„± ì‹œ ì‚¬ìš©
}

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

def get_test_transform(img_size):
    return transforms.Compose([
        transforms.Resize((img_size, img_size)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])

def get_predictions_for_single_model(model_path, model_architecture_name, num_classes, test_loader_for_model):
    """ì§€ì •ëœ ëª¨ë¸ ê²½ë¡œì™€ ì•„í‚¤í…ì²˜ë¡œ ëª¨ë¸ì„ ë¡œë“œí•˜ê³  ì˜ˆì¸¡ í™•ë¥ ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    model = CustomTimmModel(model_name=model_architecture_name, num_classes_to_predict=num_classes).to(device)
    try:
        # CPUë¡œ ëª¨ë¸ ë¡œë“œ í›„ GPUë¡œ ë³´ë‚´ëŠ” ê²ƒì´ ë©”ëª¨ë¦¬ ë¬¸ì œ ë°œìƒ ì‹œ ë„ì›€ì´ ë  ìˆ˜ ìˆìŒ
        state_dict = torch.load(model_path, map_location='cpu')
        model.load_state_dict(state_dict)
        model.to(device) # ê·¸ ë‹¤ìŒ GPUë¡œ
        print(f"Successfully loaded model: {os.path.basename(model_path)} (Arch: {model_architecture_name})")
    except Exception as e:
        print(f"Error loading model {model_path} (Arch: {model_architecture_name}): {e}")
        return None
    
    model.eval()
    model_predictions = []
    with torch.no_grad():
        for images in tqdm(test_loader_for_model, desc=f"Inferring with {os.path.basename(model_path)}", leave=False):
            images = images.to(device)
            outputs = model(images) # ë¡œì§“
            probs = F.softmax(outputs, dim=1) # í™•ë¥ 
            model_predictions.extend(probs.cpu().numpy())
    return np.array(model_predictions)

def ensemble_main():
    print("Starting Weighted Averaging Ensemble Inference...")
    print(f"Using device: {device}")
    seed_everything(CFG_ENS['SEED'])

    try:
        with open(CFG_ENS['CLASS_NAMES_PATH'], 'r') as f:
            class_names = json.load(f)
    except FileNotFoundError:
        print(f"Error: {CFG_ENS['CLASS_NAMES_PATH']} not found. Please run train.py first to generate it.")
        return
    num_classes = len(class_names)
    print(f"Loaded {num_classes} classes.") # í´ë˜ìŠ¤ ì´ë¦„ì€ ì¶œë ¥í•˜ì§€ ì•ŠìŒ (ë„ˆë¬´ ê¸¸ ìˆ˜ ìˆìŒ)

    if not CFG_ENS['MODELS_TO_ENSEMBLE']:
        print("No models specified in 'MODELS_TO_ENSEMBLE'. Please populate it.")
        return

    print(f"\nModels to be ensembled ({len(CFG_ENS['MODELS_TO_ENSEMBLE'])}):")
    for model_info in CFG_ENS['MODELS_TO_ENSEMBLE']:
        print(f"  - Path: {os.path.basename(model_info['path'])}, Arch: {model_info['name']}, ImgSize: {model_info['img_size']}, ValLogLoss: {model_info.get('val_log_loss', 'N/A')}")

    all_model_fold_predictions = []
    successfully_processed_model_infos = [] # ì„±ê³µì ìœ¼ë¡œ ì²˜ë¦¬ëœ ëª¨ë¸ì˜ ì •ë³´ë§Œ ì €ì¥
    
    current_img_size_for_loader = -1
    test_loader = None

    for model_info in CFG_ENS['MODELS_TO_ENSEMBLE']:
        model_path = model_info['path']
        model_architecture_name = model_info['name']
        model_img_size = model_info['img_size']
        
        if not os.path.exists(model_path):
            print(f"Warning: Model path {model_path} does not exist. Skipping this model.")
            continue
        
        # val_log_lossê°€ ì—†ìœ¼ë©´ ì´ ëª¨ë¸ì€ ê°€ì¤‘ í‰ê· ì—ì„œ ì œì™¸ (ë˜ëŠ” ê¸°ë³¸ ê°€ì¤‘ì¹˜ ë¶€ì—¬)
        if 'val_log_loss' not in model_info or model_info['val_log_loss'] is None:
            print(f"Warning: 'val_log_loss' not provided for {os.path.basename(model_path)}. This model might be skipped in weighted average or given default weight.")
            # ìš°ì„ ì€ ì´ëŸ° ëª¨ë¸ë„ ì˜ˆì¸¡ì€ í•˜ë„ë¡ í•˜ê³ , ê°€ì¤‘ì¹˜ ê³„ì‚° ì‹œ ì œì™¸í•˜ê±°ë‚˜ ì²˜ë¦¬
            # ì—¬ê¸°ì„œëŠ” ì¼ë‹¨ ì˜ˆì¸¡ì€ ìˆ˜í–‰í•˜ê³ , ê°€ì¤‘ì¹˜ ê³„ì‚° ì‹œì ì— val_log_lossê°€ ìˆëŠ” ëª¨ë¸ë§Œ ì‚¬ìš©

        if test_loader is None or current_img_size_for_loader != model_img_size:
            print(f"\nCreating/Recreating test_loader for img_size: {model_img_size}...")
            current_test_transform = get_test_transform(model_img_size)
            test_dataset = TestCustomImageDataset(CFG_ENS['TEST_ROOT'], transform=current_test_transform)
            if not test_dataset.samples:
                print(f"No images found in {CFG_ENS['TEST_ROOT']}. Cannot create DataLoader for img_size {model_img_size}.")
                continue 
            test_loader = DataLoader(test_dataset, batch_size=CFG_ENS['BATCH_SIZE'], shuffle=False, num_workers=2, pin_memory=True)
            current_img_size_for_loader = model_img_size
            print(f"DataLoader ready for {len(test_dataset)} test images.")
        
        fold_predictions = get_predictions_for_single_model(
            model_path, 
            model_architecture_name,
            num_classes,
            test_loader
        )
        if fold_predictions is not None:
            all_model_fold_predictions.append(fold_predictions)
            successfully_processed_model_infos.append(model_info) # ì˜ˆì¸¡ ì„±ê³µ ì‹œì—ë§Œ ì¶”ê°€
        else:
            print(f"Could not get predictions for {os.path.basename(model_path)}. Excluding from ensemble.")

    if not all_model_fold_predictions:
        print("No predictions were generated from any model. Cannot proceed with ensemble.")
        return

    # --- ê°€ì¤‘ì¹˜ ê³„ì‚° ë° ì ìš© ---
    weights_calculated = []
    valid_models_for_weighting_indices = [] # ê°€ì¤‘ì¹˜ ê³„ì‚°ì— ì‹¤ì œ ì‚¬ìš©ë  ëª¨ë¸ë“¤ì˜ ì¸ë±ìŠ¤

    for idx, model_info in enumerate(successfully_processed_model_infos):
        val_loss = model_info.get('val_log_loss')
        if val_loss is not None and val_loss > 0: # LogLossëŠ” 0ë³´ë‹¤ ì»¤ì•¼ í•¨
            # ë” ì¢‹ì€ ì„±ëŠ¥(ë‚®ì€ LogLoss)ì— ë” ë†’ì€ ê°€ì¤‘ì¹˜ë¥¼ ë¶€ì—¬í•˜ëŠ” ë°©ë²•:
            # 1. 1 / LogLoss
            # 2. max_log_loss - LogLoss (ëª¨ë“  LogLossê°€ ì–‘ìˆ˜ì´ê³ , max_log_lossë³´ë‹¤ ì‘ì„ ë•Œ)
            # 3. exp(-k * LogLoss) ë“±
            # ì—¬ê¸°ì„œëŠ” 1 / LogLoss ì‚¬ìš© (ê°„ë‹¨í•˜ê³  ì§ê´€ì )
            weight_score = 1.0 / val_loss 
            weights_calculated.append(weight_score)
            valid_models_for_weighting_indices.append(idx) # ì´ ëª¨ë¸ì˜ ì˜ˆì¸¡ì„ ê°€ì¤‘ í‰ê· ì— ì‚¬ìš©
        else:
            print(f"Warning: Skipping {os.path.basename(model_info['path'])} from weighted average due to missing or invalid val_log_loss ({val_loss}).")

    if not weights_calculated or not valid_models_for_weighting_indices:
        print("No valid weights could be calculated (e.g., all models missing val_log_loss or val_log_loss <= 0). Falling back to simple averaging for all processed models.")
        ensembled_predictions = np.mean(np.array(all_model_fold_predictions), axis=0)
    else:
        # ê°€ì¤‘ì¹˜ ê³„ì‚°ì— ì‚¬ìš©ëœ ëª¨ë¸ë“¤ì˜ ì˜ˆì¸¡ë§Œ í•„í„°ë§
        predictions_for_weighting = [all_model_fold_predictions[i] for i in valid_models_for_weighting_indices]
        
        total_weight_score = sum(weights_calculated)
        if total_weight_score == 0: # ëª¨ë“  ìœ íš¨ ê°€ì¤‘ì¹˜ê°€ 0ì¸ ê·¹ë‹¨ì  ê²½ìš° ë°©ì§€
            print("Sum of valid weight scores is zero. Falling back to simple averaging for all processed models.")
            ensembled_predictions = np.mean(np.array(all_model_fold_predictions), axis=0)
        else:
            normalized_weights = [w / total_weight_score for w in weights_calculated]
            
            print(f"\nApplying Weighted Averaging using {len(normalized_weights)} models with valid val_log_loss:")
            for i, model_idx in enumerate(valid_models_for_weighting_indices):
                model_info = successfully_processed_model_infos[model_idx]
                print(f"  - {os.path.basename(model_info['path'])} (ValLoss: {model_info['val_log_loss']:.4f}): Weight = {normalized_weights[i]:.4f}")
            
            ensembled_predictions = np.average(np.array(predictions_for_weighting), axis=0, weights=normalized_weights)
            print(f"\nSuccessfully applied weighted averaging.")

    # ì œì¶œ íŒŒì¼ ìƒì„±
    pred_df = pd.DataFrame(ensembled_predictions, columns=class_names)

    try:
        submission_df = pd.read_csv(CFG_ENS['SAMPLE_SUBMISSION_PATH'], encoding='utf-8-sig')
    except FileNotFoundError:
        print(f"Error: Sample submission file {CFG_ENS['SAMPLE_SUBMISSION_PATH']} not found.")
        return

    class_columns_in_submission = submission_df.columns[1:]
    try:
        submission_df[class_columns_in_submission] = pred_df[class_columns_in_submission].values
    except KeyError as e:
        print(f"KeyError: Mismatch between submission file columns and predicted class names: {e}")
        print(f"Please ensure your class_names.json (derived from training) matches the submission format.")
        return
    except ValueError as e:
        print(f"ValueError during submission assignment (likely shape mismatch - number of test images or classes): {e}")
        return
    
    output_submission_filename = f'weight_emsemble_v8.csv'
    submission_df.to_csv(output_submission_filename, index=False, encoding='utf-8-sig')
    print(f"ğŸ‰ Weighted ensemble submission file created successfully: {output_submission_filename}")

if __name__ == '__main__':
    # ì´ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‹¤í–‰í•˜ê¸° ì „ì— CFG_ENS['MODELS_TO_ENSEMBLE'] ì•ˆì˜
    # ê° ëª¨ë¸ ë”•ì…”ë„ˆë¦¬ì— 'val_log_loss' í‚¤ì™€ ì‹¤ì œ ê²€ì¦ LogLoss ê°’ì„ ì¶”ê°€í•´ì•¼ í•©ë‹ˆë‹¤.
    ensemble_main()