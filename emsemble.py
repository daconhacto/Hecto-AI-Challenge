import os
import pandas as pd
import numpy as np
import json
import torch
from torch.utils.data import Dataset, DataLoader
from tqdm import tqdm
import torchvision.transforms as transforms
import torch.nn.functional as F
from PIL import Image
import timm # pip install timm
import random

CFG = {
    # --- ê³µí†µ ì„¤ì • ---
    'BATCH_SIZE': 16,
    'SEED': 42,
    'CLASS_NAMES_PATH': './class_names.json',  # train.py ì‹¤í–‰ í›„ ìƒì„±ëœ class_names.json ê²½ë¡œ
    'TEST_ROOT': './data/test',                    # í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ í´ë” ê²½ë¡œ
    'SAMPLE_SUBMISSION_PATH': './data/sample_submission.csv', # ìƒ˜í”Œ ì œì¶œ íŒŒì¼ ê²½ë¡œ

    # --- 1ë‹¨ê³„ ì¶”ë¡  ë° ì•™ìƒë¸” ì„¤ì • ---
    'STAGE1_GROUPS': [
        {
            "group_name": "ConvNext Ensemble",
            "output_csv_path": "stage1_convnext_ensemble.csv",
            # ì´ ê·¸ë£¹ ì „ì²´ì˜ ëŒ€í‘œ LogLoss ê°’ (2ë‹¨ê³„ ì•™ìƒë¸” ê°€ì¤‘ì¹˜ ê³„ì‚°ì— ì‚¬ìš©)
            "ensemble_log_loss": 0.09121,
            "models": [
                # ConvNext 5ê°œ Foldì— ëŒ€í•œ ì •ë³´ë¥¼ ì…ë ¥í•˜ì„¸ìš”.
                {"path": "./pth_files/conv_fold1.pth", "name": "convnext_large_mlp.clip_laion2b_augreg_ft_in1k_384", "img_size": 600, "val_log_loss": 0.09476},
                {"path": "./pth_files/conv_fold2.pth", "name": "convnext_large_mlp.clip_laion2b_augreg_ft_in1k_384", "img_size": 600, "val_log_loss": 0.10195},
                {"path": "./pth_files/conv_fold3.pth", "name": "convnext_large_mlp.clip_laion2b_augreg_ft_in1k_384", "img_size": 600, "val_log_loss": 0.10860},
                {"path": "./pth_files/conv_fold4.pth", "name": "convnext_large_mlp.clip_laion2b_augreg_ft_in1k_384", "img_size": 600, "val_log_loss": 0.10146},
                {"path": "./pth_files/conv_fold5.pth", "name": "convnext_large_mlp.clip_laion2b_augreg_ft_in1k_384", "img_size": 600, "val_log_loss": 0.10181},
            ]
        },
        {
            "group_name": "EVA Ensemble",
            "output_csv_path": "stage1_eva_ensemble.csv",
            # ì´ ê·¸ë£¹ ì „ì²´ì˜ ëŒ€í‘œ LogLoss ê°’ (2ë‹¨ê³„ ì•™ìƒë¸” ê°€ì¤‘ì¹˜ ê³„ì‚°ì— ì‚¬ìš©)
            "ensemble_log_loss": 0.10367,
            "models": [
                # EVA 3ê°œ Foldì— ëŒ€í•œ ì •ë³´ë¥¼ ì…ë ¥í•˜ì„¸ìš”. (ê°€ì¤‘ì¹˜: 0.13874, 0.112754, 0.1089)
                {"path": "./pth_files/eva_fold1.pth", "name": "eva02_large_patch14_448.mim_m38m_ft_in1k", "img_size": 448, "val_log_loss": 0.13874},
                {"path": "./pth_files/eva_fold2.pth", "name": "eva02_large_patch14_448.mim_m38m_ft_in1k", "img_size": 448, "val_log_loss": 0.12754},
                {"path": "./pth_files/eva_fold3.pth", "name": "eva02_large_patch14_448.mim_m38m_ft_in1k", "img_size": 448, "val_log_loss": 0.11089},
            ]
        }
    ],

    # --- 2ë‹¨ê³„ CSV ì•™ìƒë¸” ì„¤ì • ---
    'STAGE2_OUTPUT_NAME': 'final_submission.csv',
}

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

def seed_everything(seed):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

class CustomTimmModel(torch.nn.Module):
    def __init__(self, model_name, num_classes_to_predict, pretrained=True):
        super(CustomTimmModel, self).__init__()
        self.backbone = timm.create_model(model_name, pretrained=pretrained, num_classes=0, global_pool='avg')
        self.feature_dim = self.backbone.num_features
        self.head = torch.nn.Linear(self.feature_dim, num_classes_to_predict)
    def forward(self, x):
        features = self.backbone(x)
        output = self.head(features)
        return output

class TestCustomImageDataset(Dataset):
    def __init__(self, root_dir, transform=None):
        self.root_dir = root_dir
        self.transform = transform
        self.samples = []
        if not os.path.exists(root_dir) or not os.path.isdir(root_dir):
            print(f"ğŸš¨ ê²½ê³ : í…ŒìŠ¤íŠ¸ ë””ë ‰í† ë¦¬ {root_dir}ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return
        for fname in sorted(os.listdir(root_dir)):
            if fname.lower().endswith(('.jpg', '.jpeg', '.png')):
                img_path = os.path.join(root_dir, fname)
                self.samples.append((img_path,))
    def __len__(self):
        return len(self.samples)
    def __getitem__(self, idx):
        img_path = self.samples[idx][0]
        image = Image.open(img_path).convert('RGB')
        if self.transform:
            image = self.transform(image)
        return image

def get_test_transform(img_size):
    return transforms.Compose([
        transforms.Resize((img_size, img_size)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])

def compute_weights(items_with_loss, loss_key='val_log_loss'):
    """val_log_loss ë¦¬ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê°€ì¤‘ì¹˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤."""
    losses = [info.get(loss_key) for info in items_with_loss]
    if any(l is None or l <= 0 for l in losses):
        print("âš ï¸ val_log_lossê°€ ì—†ê±°ë‚˜ 0 ì´í•˜ì¸ í•­ëª©ì´ ìˆì–´ ë™ì¼ ê°€ì¤‘ì¹˜ë¥¼ ì ìš©í•©ë‹ˆë‹¤.")
        return np.ones(len(items_with_loss)) / len(items_with_loss)
    
    inv_losses = 1.0 / np.array(losses)
    weights = inv_losses / inv_losses.sum()
    return weights

def run_stage1_inference_ensemble(group_config, num_classes):
    """ì§€ì •ëœ ëª¨ë¸ ê·¸ë£¹ì— ëŒ€í•œ ì¶”ë¡  ë° ê°€ì¤‘ í‰ê·  ì•™ìƒë¸”ì„ ìˆ˜í–‰í•˜ê³  ê²°ê³¼ë¥¼ CSVë¡œ ì €ì¥í•©ë‹ˆë‹¤."""
    print("\n" + "="*80)
    print(f"ğŸš€ STAGE 1: '{group_config['group_name']}' ê·¸ë£¹ ì²˜ë¦¬ ì‹œì‘")
    print("="*80)

    all_model_predictions = []
    
    current_img_size_for_loader = -1
    test_loader = None

    for model_info in group_config['models']:
        model_path = model_info['path']
        model_arch = model_info['name']
        model_img_size = model_info['img_size']

        if not os.path.exists(model_path):
            print(f"ğŸš¨ ê²½ê³ : ëª¨ë¸ ê²½ë¡œ {model_path}ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì´ ëª¨ë¸ì€ ê±´ë„ˆëœë‹ˆë‹¤.")
            continue

        # ì´ë¯¸ì§€ í¬ê¸°ì— ë§ëŠ” DataLoader ìƒì„± ë˜ëŠ” ì¬ì‚¬ìš©
        if test_loader is None or current_img_size_for_loader != model_img_size:
            print(f"\nğŸ”„ ì´ë¯¸ì§€ í¬ê¸° {model_img_size}ì— ëŒ€í•œ DataLoaderë¥¼ ìƒì„±í•©ë‹ˆë‹¤...")
            transform = get_test_transform(model_img_size)
            dataset = TestCustomImageDataset(CFG['TEST_ROOT'], transform=transform)
            if not dataset.samples:
                raise FileNotFoundError(f"í…ŒìŠ¤íŠ¸ í´ë” '{CFG['TEST_ROOT']}'ì— ì´ë¯¸ì§€ê°€ ì—†ìŠµë‹ˆë‹¤.")
            test_loader = DataLoader(dataset, batch_size=CFG['BATCH_SIZE'], shuffle=False, num_workers=2, pin_memory=True)
            current_img_size_for_loader = model_img_size
            print(f"âœ… DataLoader ì¤€ë¹„ ì™„ë£Œ. ({len(dataset)}ê°œ ì´ë¯¸ì§€)")

        # ë‹¨ì¼ ëª¨ë¸ ì¶”ë¡ 
        print(f"  - ì¶”ë¡  ëª¨ë¸: {os.path.basename(model_path)} (Arch: {model_arch}, ImgSize: {model_img_size})")
        model = CustomTimmModel(model_name=model_arch, num_classes_to_predict=num_classes).to(device)
        try:
            state_dict = torch.load(model_path, map_location='cpu')
            model.load_state_dict(state_dict)
            model.to(device)
        except Exception as e:
            print(f"ğŸš¨ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}. ì´ ëª¨ë¸ì€ ê±´ë„ˆëœë‹ˆë‹¤.")
            continue
        
        model.eval()
        model_preds = []
        with torch.no_grad():
            for images in tqdm(test_loader, desc=f"   Inferring", leave=False):
                images = images.to(device)
                outputs = model(images)
                probs = F.softmax(outputs, dim=1)
                model_preds.extend(probs.cpu().numpy())
        
        all_model_predictions.append(np.array(model_preds))

    if not all_model_predictions:
        print(f"ğŸš¨ '{group_config['group_name']}' ê·¸ë£¹ì—ì„œ ìœ íš¨í•œ ì˜ˆì¸¡ì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        return None

    # ê·¸ë£¹ ë‚´ ëª¨ë¸ë“¤ì˜ ê°€ì¤‘ í‰ê·  ì•™ìƒë¸”
    print("\nâš–ï¸ ê·¸ë£¹ ë‚´ ê°€ì¤‘ í‰ê·  ì•™ìƒë¸”ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤...")
    weights = compute_weights(group_config['models'])
    
    print("  - ê³„ì‚°ëœ ê°€ì¤‘ì¹˜:")
    for i, model_info in enumerate(group_config['models']):
        print(f"    - {os.path.basename(model_info['path'])} (Loss: {model_info['val_log_loss']:.5f}): {weights[i]:.4f}")

    ensembled_predictions = np.average(np.array(all_model_predictions), axis=0, weights=weights)

    # ê²°ê³¼ë¥¼ CSVë¡œ ì €ì¥
    try:
        submission_df = pd.read_csv(CFG['SAMPLE_SUBMISSION_PATH'])
        class_names = submission_df.columns[1:].tolist()
        pred_df = pd.DataFrame(ensembled_predictions, columns=class_names)
        submission_df[class_names] = pred_df[class_names].values
        
        output_path = group_config["output_csv_path"]
        submission_df.to_csv(output_path, index=False, encoding='utf-8-sig')
        print(f"\nâœ… STAGE 1 ì™„ë£Œ. ê²°ê³¼ ì €ì¥: {output_path}")
        return {
            "path": output_path,
            "val_log_loss": group_config["ensemble_log_loss"]
        }
    except Exception as e:
        print(f"ğŸš¨ CSV íŒŒì¼ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None

def run_stage2_csv_ensemble(csv_infos):
    """1ë‹¨ê³„ì—ì„œ ìƒì„±ëœ CSVë“¤ì„ ê°€ì¤‘ í‰ê· í•˜ì—¬ ìµœì¢… ì œì¶œ íŒŒì¼ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    print("\n" + "="*80)
    print(f"ğŸ“¦ STAGE 2: CSV íŒŒì¼ ì•™ìƒë¸” ì‹œì‘")
    print("="*80)

    try:
        dfs = [pd.read_csv(info["path"]) for info in csv_infos]
    except FileNotFoundError as e:
        print(f"ğŸš¨ ì•™ìƒë¸”í•  CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
        return
        
    # ê°€ì¤‘ì¹˜ ê³„ì‚°
    print("âš–ï¸ 2ë‹¨ê³„ ì•™ìƒë¸” ê°€ì¤‘ì¹˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤...")
    weights = compute_weights(csv_infos, loss_key='val_log_loss')
    
    print("  - ê³„ì‚°ëœ ê°€ì¤‘ì¹˜:")
    for i, info in enumerate(csv_infos):
        print(f"    - {os.path.basename(info['path'])} (Group Loss: {info['val_log_loss']:.5f}): {weights[i]:.4f}")

    # í™•ë¥  ê°’ ì¶”ì¶œ ë° ì•™ìƒë¸”
    prob_cols = dfs[0].columns[1:]
    probs_stack = np.stack([df[prob_cols].values for df in dfs], axis=0)
    final_probs = np.average(probs_stack, axis=0, weights=weights)

    # ìµœì¢… ì œì¶œ íŒŒì¼ ìƒì„±
    final_submission_df = pd.read_csv(CFG['SAMPLE_SUBMISSION_PATH'])
    final_submission_df[prob_cols] = final_probs
    
    output_path = CFG['STAGE2_OUTPUT_NAME']
    final_submission_df.to_csv(output_path, index=False, encoding='utf-8-sig')
    print(f"\nğŸ‰ ìµœì¢… ì•™ìƒë¸” ì™„ë£Œ! ì œì¶œ íŒŒì¼ ì €ì¥: {output_path}")

def main():
    print("ğŸŒŸ 2-Stage Weighted Ensemble Scriptë¥¼ ì‹œì‘í•©ë‹ˆë‹¤. ğŸŒŸ")
    print(f"ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")
    seed_everything(CFG['SEED'])

    try:
        with open(CFG['CLASS_NAMES_PATH'], 'r') as f:
            class_names = json.load(f)
        num_classes = len(class_names)
        print(f"í´ë˜ìŠ¤ ì •ë³´ ë¡œë“œ ì™„ë£Œ ({num_classes}ê°œ í´ë˜ìŠ¤).")
    except FileNotFoundError:
        print(f"ğŸš¨ ì¹˜ëª…ì  ì˜¤ë¥˜: {CFG['CLASS_NAMES_PATH']} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print("   ë¨¼ì € train.pyë¥¼ ì‹¤í–‰í•˜ì—¬ í´ë˜ìŠ¤ íŒŒì¼ì„ ìƒì„±í•´ì•¼ í•©ë‹ˆë‹¤.")
        return

    # --- STAGE 1 ì‹¤í–‰ ---
    stage1_results = []
    for group_config in CFG['STAGE1_GROUPS']:
        result_info = run_stage1_inference_ensemble(group_config, num_classes)
        if result_info:
            stage1_results.append(result_info)
    
    if len(stage1_results) != len(CFG['STAGE1_GROUPS']):
        print("\nğŸš¨ 1ë‹¨ê³„ì—ì„œ ì¼ë¶€ ê·¸ë£¹ ì²˜ë¦¬ì— ì‹¤íŒ¨í•˜ì—¬ 2ë‹¨ê³„ ì•™ìƒë¸”ì„ ì§„í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
        
    # --- STAGE 2 ì‹¤í–‰ ---
    run_stage2_csv_ensemble(stage1_results)

if __name__ == '__main__':
    main()
